<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>Hongbo&#39;s Design</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="human-computer interaction">
  

  <!--Author-->
  
      <meta name="author" content="John Doe">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  
<link rel="stylesheet" href="/hongbo.github.io/css/styles.css">


  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics -->
  

<meta name="generator" content="Hexo 6.3.0"></head>


  <body>
  	<div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/hongbo.github.io/">Hongbo's Design</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
        <li class="nav-item">
          <a class="nav-link" href="/hongbo.github.io/about%20hongbo">hongbo&#39;s info</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="/hongbo.github.io/hongbo's%20research">hongbo&#39;s research</a>
        </li>
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			
<div class="col-12 pt-lg-3 mb-4 px-4 portfolio-content">
  <p>üßë‚Äçüéì I am a doctoral student at the <em>College of Computer Science and Technology</em> and  the <em>International Design Institute</em> of <em><a target="_blank" rel="noopener" href="https://www.zju.edu.cn/english/">Zhejiang University</a>, with Prof. <a target="_blank" rel="noopener" href="https://person.zju.edu.cn/en/lingyun">Lingyun Sun</a>.</em> </p>
<p>üîç I researched Human-AI Co-creation, Natural Human-computer Interaction, Hybrid Augmented Interation and Multi-modal Interaction.</p>
<hr>
<h5 id="Research-Interest-1-Natural-Human-Computer-Interaction"><a href="#Research-Interest-1-Natural-Human-Computer-Interaction" class="headerlink" title="Research Interest 1: Natural Human-Computer Interaction"></a>Research Interest 1: <strong>Natural Human-Computer Interaction</strong></h5><p style="line-height:30px; text-indent:2em; text-align:justify;"> Explore natural and direct human-computer interaction modes, including body-based interaction, multi-sensory interaction, XR interaction and so on. Improve the user experience, efficiency, and intuition through interaction mode innovation. </p>

<ul>
<li>Example Research: <em>Elicitation and Evaluation of Hand-based Interaction Language for 3D Conceptual Design in Mixed Reality</em></li>
</ul>
<img src="./images/gesture.png" style="zoom:28%;" />

<p style="line-height:30px; text-indent:2em; text-align:justify;"> This study aims to establish a set of hand-based languages that are available in cognitive, physical, and system-based aspects for 3D conceptual designs in the MR environment. A two-stage study was conducted in an MR environment, combining an elicitation design and a comprehensive evaluation experiment. Through the elicitation design, approximately 930 gesture actions, focusing on 31 targeted functionalities, were collected. By performing an evaluation experiment, a set of theoretically optimal hand-based interaction languages for 3D conceptual design was proposed, and the corresponding guidelines for hand-based interactions were clarified. Our results can further expand human-computer interactions in MR environments and inspire software builders to create novel hand-driven interaction modes or tools. </p>


<p> &emsp; üü© International Journal of Human-Computer Studies (IJHCS) | Q1 &amp; CCF A | First Student Author | <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1071581923002070">DOI</a> | <a target="_blank" rel="noopener" href="http://localhost:4000/hongbo.github.io/images/pdf2.pdf">PDF</a> </p>
<p style="line-height:50px;">   &#8194  </p> 

<ul>
<li>Example Research: <em>Effect of Gesture Type and Operating Configuration on Performance, Physiological Load, and Subjective Evaluation</em></li>
</ul>
<img src="./images/gesture2.png" style="zoom:28%;" />


<p style="line-height:30px; text-indent:2em; text-align:justify;"> This study investigate the influence of four commonly-selected tablet configurations and seven touchscreen gestures on electromyography, performance, and subjective assessment. Our results indicated that muscular loads of shoulder decreased under the Stand-Hand configuration while it increased under the Sit-Table during gesture interaction. We also found that Drag-Up and Drag-Left tended to possess higher muscular loads of shoulder while Drag-Down caused greater muscular loads of index finger. Besides, two-touch gestures spent longer duration when performing long-distance movements. Our findings could provide a scientific basis for guiding the appropriate selection and the use of touchscreen interaction in the future HCI field. </p>

<p>&emsp; üü© International Journal of Human-Computer Interaction (IJHCI) | Q1 &amp; CCF B |First Student Author | <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10108654">DOI</a> | <a target="_blank" rel="noopener" href="http://localhost:4000/hongbo.github.io/images/pdf1.pdf">PDF</a></p>
<p style="line-height:50px;">   &#8194  </p> 

<ul>
<li>Example Research: <em>Designing a Smart VR Painting System with Multisensory Interaction for Immersive Experience</em></li>
</ul>
<img src="./images/VR.png" style="zoom:28%;" />

<p style="line-height:30px; text-indent:2em; text-align:justify;"> This study developed Painting++, a multisensory VR painting system which synthetically integrates visual, audio, haptic, and smell feedback to create a sense of immersion. Particularly, the content generation ability of AI is also adopted to assist users in obtaining completed stereo paintings from rough sketches. The results illustrated that our system succeeded in creating highly immersive painting experience. In addition, the assistance of AI significantly improved the ease of use and effectiveness of VR painting. Our work demonstrates the value of multisensory interaction and the AI capability in enriching VR painting experience. </p>

<p>&emsp; üü© IEEE VR Conference | CCF A |Second Student Author | <a target="_blank" rel="noopener" href="https://doi.org/10.1080/10447318.2022.2111051">DOI</a> | <a target="_blank" rel="noopener" href="http://localhost:4000/hongbo.github.io/images/pdf3.pdf">PDF</a> </p>
<p style="line-height:50px;">   &#8194  </p> 

<h5 id="Research-Interest-2-Human-AI-Collaborative-Interaction-Human-AI-Co-creation"><a href="#Research-Interest-2-Human-AI-Collaborative-Interaction-Human-AI-Co-creation" class="headerlink" title="Research Interest 2: Human-AI Collaborative Interaction &amp; Human-AI Co-creation"></a>Research Interest 2: <strong>Human-AI Collaborative Interaction &amp; Human-AI Co-creation</strong></h5><p>Explore the mechanism of Human-AI communication and cooperation. Focus on the role, cooperation mode, and contribution type of AI agent in the co-creation process. Develop the co-creation system and establish the shared mental model between human and AI agent.</p>
<ul>
<li>Example Research: <em>A Hybrid Prototype Method Combining Physical Models and Generative Artificial Intelligence to Support Creativity in Conceptual Design</em></li>
</ul>
<img src="./images/tochi.png" style="zoom:28%;" />

<p style="line-height:30px; text-indent:2em; text-align:justify;"> Large-scale generation models are able to offer data-enabled creativity support through generating high-quality solutions comparable to human designers, which opens up an imaginary space for designers and brings new possibilities for creativity support tools. In this study, A hybrid prototype method combining tangible models and generative artificial intelligence (AI) synergistically was proposed for creativity support in the conceptual design stage. A hybrid prototype system was developed to implement the proposed method. We clarified the contributions and limitations of the hybrid prototype method and discussed value and challenges of collaboration with generative AI. </p>

<p>&emsp; üü• ACM transactions on computer-human interaction (TOCHI) | CCF A | First Author | Under Review After Revision</p>
<p style="line-height:50px;">   &#8194  </p> 

<ul>
<li>Example Research: <em>A Hybrid Prototype Method Combining Physical Models and Generative Artificial Intelligence to Support Creativity in Conceptual Design</em></li>
</ul>
<img src="./images/chi.png" style="zoom:28%;" />

<p style="line-height:30px; text-indent:2em; text-align:justify;"> Although recent advancements in generative AI makes AI possible to be a co-creator in prototyping process, designers still think AI is a challenging design material, because they always need to follow computer-centered rules to interact with it instead of using their familiar design materials and languages. In this study, we propose ProtoDreamer, a mixed-prototype tool that synergizes generative AI with physical prototype to facilitate conceptual design. ProtoDreamer allows designers to construct preliminary prototypes using physical materials, while AI recognizes these forms and vocal inputs to generate diverse design alternatives.  ProtoDreamer supports designers to express design intentions to AI with familiar design language and derive inspirations through incessant generated artifacts. </p>

<p>ProtoDreamer allows designers to construct preliminary prototypes using physical materials, while AI recognizes these forms and vocal inputs to generate diverse design alternatives.  ProtoDreamer supports designers to express design intentions to AI with familiar design language and derive inspirations through incessant generated artifacts.<br>&emsp; üü• ACM CHI Virtual Conference on Human Factors in Computing Systems (CHI) | CCF A | First Author | Under Review After Revision</p>

</div>


      </div>
      
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>

  </body>
</html>
